{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código Base (Inspiração)\n",
    "\n",
    "* link: https://goodboychan.github.io/python/tensorflow/mit/2021/02/14/music-generation.html\n",
    "* link2: https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab1/solutions/Part2_Music_Generation_Solution.ipynb\n",
    "* link3 (irish dataset): https://github.com/aamini/introtodeeplearning/tree/master/mitdeeplearning/data\n",
    "* some articles like those in biblio.bib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # tf warning\n",
    "\n",
    "# libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import regex as re\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from music21 import converter\n",
    "\n",
    "# fixa para executar na CPU (determinismo)\n",
    "devices = tf.config.list_physical_devices()\n",
    "tf.config.experimental.set_visible_devices(devices[0]) \n",
    "\n",
    "# semente aleatória\n",
    "global_seed=301831\n",
    "np.random.seed(global_seed)\n",
    "random.seed(global_seed)\n",
    "tf.random.set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_snippet(text):\n",
    "    pattern = '(^|\\n\\n)(.*?)\\n\\n'\n",
    "    search_results = re.findall(pattern, text, overlapped=True, flags=re.DOTALL)\n",
    "    songs = [song[1] for song in search_results]\n",
    "    # print(\"Found {} songs in text\".format(len(songs)))\n",
    "    return songs\n",
    "\n",
    "cwd = os.getcwd()\n",
    "songs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(cwd, 'data', 'irish', 'irish.abc'), 'r') as f:\n",
    "#     text = f.read()\n",
    "#     songs = extract_song_snippet(text)\n",
    "\n",
    "# # Exemplo de musica\n",
    "# example_song = songs[0]\n",
    "# print(\"\\nExample song: \")\n",
    "# print(example_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC Notation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def abc_filenames(datapath='abcnotation_midi-test', count=False):\n",
    "#     songs = [os.path.join(datapath, f) for f in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, f))]\n",
    "#     if count==True:\n",
    "#         print('Found {} songs in directory'.format(len(songs)))\n",
    "#     return songs\n",
    "\n",
    "# datapath = \"./data/abcnotation\"\n",
    "# filenames = abc_filenames(datapath, count=False)\n",
    "# idx = np.random.choice(len(filenames), 5000, replace=False)\n",
    "# filenames_selected = [filenames[i] for i in idx]\n",
    "# songs_text = ''\n",
    "# for filename in filenames_selected: \n",
    "#     try:\n",
    "#             f = open(filename,'r', encoding='utf8')\n",
    "#             text = f.read()\n",
    "#             text = re.findall('X:.*', text, overlapped=True, flags=re.DOTALL)[0]\n",
    "#             songs_text += text\n",
    "#             f.close()\n",
    "#     except:\n",
    "#         print(\"Erro em:\", filename)\n",
    "#         pass\n",
    "\n",
    "\n",
    "# with open(os.path.join(cwd, 'data', 'abcnotation_sample', 'abcnotation_sample.abc'), 'w') as f:\n",
    "#     text = f.write(songs_text)\n",
    "#     f.close()\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(cwd, 'data', 'abcnotation_sample', 'abcnotation_sample.abc'), 'r') as f:\n",
    "    text = f.read()\n",
    "    songs = extract_song_snippet(text)\n",
    "\n",
    "# # Exemplo de musica\n",
    "# example_song = songs[0]\n",
    "# print(\"\\nExample song: \")\n",
    "# print(example_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_abc(abc_song, show_rslt=False):\n",
    "    # original .abc\n",
    "    text = abc_song\n",
    "    new_text = text\n",
    "\n",
    "    # # remove index field\n",
    "    # new_text = re.sub('(X:).*\\n', '', new_text)\n",
    "\n",
    "    # remove title field\n",
    "    new_text = re.sub('(T:).*\\n', '', new_text)\n",
    "\n",
    "    # remove composer field\n",
    "    new_text = re.sub('(C:).*\\n', '', new_text)\n",
    "\n",
    "    # remove source field\n",
    "    new_text = re.sub('(S:).*\\n', '', new_text)\n",
    "\n",
    "    # remove geographical origin field\n",
    "    new_text = re.sub('(O:).*\\n', '', new_text)\n",
    "\n",
    "    # remove textual notes field\n",
    "    new_text = re.sub('(N:).*\\n', '', new_text)\n",
    "\n",
    "    # remove identity of transcriber/source of transcription field\n",
    "    new_text = re.sub('(Z:).*\\n', '', new_text)\n",
    "\n",
    "    # remove lyrics\n",
    "    new_text = re.sub('([wW]:).*\\n','',new_text)\n",
    "    new_text = re.sub('[wW]:','',new_text)\n",
    "\n",
    "\n",
    "    # remove comments\n",
    "    new_text = re.sub('(%).*\\n','',new_text)\n",
    "\n",
    "    # remove other fields\n",
    "    new_text = re.sub('(R:).*\\n','',new_text)\n",
    "    new_text = re.sub('(I:).*\\n','',new_text)\n",
    "    new_text = re.sub('(B:).*\\n','',new_text)\n",
    "\n",
    "    if show_rslt==True:\n",
    "        # print results\n",
    "        print('original: '+str(len(text))+' characters'+'\\n' + text)\n",
    "        print()\n",
    "        print('after modification: '+str(len(new_text))+' characters'+'\\n' + new_text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "# example_song2 = clean_abc(songs[3], show_rslt=True)\n",
    "# print(example_song2)\n",
    "\n",
    "for i in range(len(songs)):\n",
    "    songs[i] = clean_abc(songs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab(text):\n",
    "    vocab = sorted(set(text))\n",
    "    return {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "songs_joined = \"\\n\\n\".join(songs) \n",
    "\n",
    "# caracteres únicos disponíveis\n",
    "vocab = extract_vocab(songs_joined)\n",
    "\n",
    "print('Total Characters:', len(songs_joined))\n",
    "print('Total Vocab:', len(vocab))\n",
    "\n",
    "songs_train, songs_test = train_test_split(songs, train_size = 0.80)\n",
    "songs_train_joined = \"\\n\\n\".join(songs_train)\n",
    "songs_test_joined = \"\\n\\n\".join(songs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from character to unique index\n",
    "def char2idx(string, vocab=None):\n",
    "    if vocab==None:\n",
    "        vocab = extract_vocab(string)\n",
    "    vectorized_list = np.array([vocab[s] for s in string])\n",
    "    return vectorized_list\n",
    "\n",
    "# Create a mapping from indices to characters\n",
    "def idx2char(idx, vocab):\n",
    "    keys = list(vocab.keys())\n",
    "    string = ''\n",
    "    if isinstance(idx, collections.Iterable):\n",
    "        for i in idx:\n",
    "            string += keys[i]\n",
    "    else:\n",
    "        string += keys[idx]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando Dados para Treino (Ajuste) do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(vectorized_string, seq_length, method, batch_size=None, vocab=None):\n",
    "\n",
    "  # Todas as Combinações + One Hot Encoding (Dummies)\n",
    "  if method == 'full':\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    n = len(vectorized_string)\n",
    "\n",
    "    for i in range(0, n-seq_length, 1):\n",
    "      seq_in = vectorized_string[i:i + seq_length]\n",
    "      seq_out = vectorized_string[i + seq_length]\n",
    "      x_train.append(seq_in)\n",
    "      y_train.append(seq_out)\n",
    "\n",
    "    n_patterns = len(x_train)\n",
    "\n",
    "    x_train = np.reshape(x_train, (n_patterns, seq_length, 1))\n",
    "    # normalize\n",
    "    x_train = x_train / float(len(vocab))\n",
    "    # one hot encode the output variable\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "\n",
    "  # Lotes com tamanho específico + Word Embedding\n",
    "  if method == 'batch':\n",
    "    n = len(vectorized_string) - 1\n",
    "    idx = np.random.choice(n-seq_length, batch_size)\n",
    "    seq_in = [vectorized_string[i:i+seq_length] for i in idx]\n",
    "    seq_out = [vectorized_string[i+1: i+1+seq_length] for i in idx]\n",
    "\n",
    "    x_train = np.reshape(seq_in, [batch_size, seq_length])\n",
    "    y_train = np.reshape(seq_out, [batch_size, seq_length])\n",
    "\n",
    "  return x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_wordEmbed(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    # Layer 1: Embedding layer to transform indices into dense vectors \n",
    "    #   of a fixed embedding size\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "\n",
    "    # Layer 2: LSTM with `rnn_units` number of units. \n",
    "    tf.keras.layers.LSTM(\n",
    "    units=rnn_units, \n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    recurrent_initializer='glorot_uniform',\n",
    "    use_bias=True,\n",
    "    bias_initializer='zeros',\n",
    "    return_sequences=True, \n",
    "    stateful=True\n",
    "    ),\n",
    "\n",
    "    # Layer 3: Dense (fully-connected) layer that transforms the LSTM output\n",
    "    #   into the vocabulary size. \n",
    "    tf.keras.layers.Dense(units=vocab_size)\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def build_model_oneHotEnc(rnn_units, x_train, y_train):\n",
    "  model = tf.keras.Sequential([\n",
    "    # Layer 1: LSTM with `rnn_units` number of units. \n",
    "    tf.keras.layers.LSTM(\n",
    "    units=rnn_units,\n",
    "    input_shape=(x_train.shape[1], x_train.shape[2]), \n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    recurrent_initializer='glorot_uniform',\n",
    "    use_bias=True,\n",
    "    bias_initializer='zeros',\n",
    "    return_sequences=False, \n",
    "    stateful=False,\n",
    "    ),\n",
    "\n",
    "    # Layer 2: Dense (fully-connected) layer that transforms the LSTM output\n",
    "    #   into the vocabulary size. \n",
    "    tf.keras.layers.Dense(units=y_train.shape[1], activation='softmax')\n",
    "  ])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_wordEmbed(labels, logits):\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return loss\n",
    "\n",
    "def train_step_wordEmbed(x, y): \n",
    "  with tf.GradientTape() as tape:\n",
    "    y_hat = model(x)\n",
    "    loss = compute_loss_wordEmbed(y, y_hat)\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_variables)  \n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss\n",
    "  \n",
    "\n",
    "class PeriodicPlotter:\n",
    "  def __init__(self, sec, xlabel='', ylabel='', scale=None):\n",
    "\n",
    "    self.xlabel = xlabel\n",
    "    self.ylabel = ylabel\n",
    "    self.sec = sec\n",
    "    self.scale = scale\n",
    "\n",
    "    self.tic = time.time()\n",
    "\n",
    "  def plot(self, data):\n",
    "    if time.time() - self.tic > self.sec:\n",
    "      plt.cla()\n",
    "\n",
    "      if self.scale is None:\n",
    "        plt.plot(data)\n",
    "      elif self.scale == 'semilogx':\n",
    "        plt.semilogx(data)\n",
    "      elif self.scale == 'semilogy':\n",
    "        plt.semilogy(data)\n",
    "      elif self.scale == 'loglog':\n",
    "        plt.loglog(data)\n",
    "      else:\n",
    "        raise ValueError(\"unrecognized parameter scale {}\".format(self.scale))\n",
    "\n",
    "      plt.xlabel(self.xlabel); plt.ylabel(self.ylabel)\n",
    "      ipythondisplay.clear_output(wait=True)\n",
    "      ipythondisplay.display(plt.gcf())\n",
    "\n",
    "      self.tic = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epochs = 2000  # Increase this to train longer\n",
    "batch_size = 4 # [4,16]  # Experiment between 1 and 64\n",
    "seq_length = 50 # [50,200]  # Experiment between 50 and 500\n",
    "learning_rate = 1e-3 # [1e-3,1e-5]  # Experiment between 1e-5 and 1e-1 \n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 # [256, 512] # Experiment between 8 and 1024\n",
    "lstm_units = 256 # [256, 1024]  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'my_ckpt')\n",
    "\n",
    "# train | test\n",
    "vec_songs_train = char2idx(songs_train_joined, vocab)\n",
    "vec_songs_test = char2idx(songs_test_joined, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch + Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_wordEmbed(vocab_size, embedding_dim, lstm_units, batch_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Begin training! #\n",
    "###################\n",
    "\n",
    "train_loss_history = []\n",
    "plotter = PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "for iter in tqdm(range(epochs)):\n",
    "\n",
    "    # Grab a batch and propagate it through the network\n",
    "    x_train, y_train = prepare_train_data(vectorized_string=vec_songs_train, seq_length=seq_length, method='batch', batch_size=batch_size)\n",
    "    train_loss = train_step_wordEmbed(x_train, y_train)\n",
    "    train_loss = train_loss.numpy().mean()\n",
    "\n",
    "    # Update the progress bar\n",
    "    train_loss_history.append(train_loss)\n",
    "    plotter.plot(train_loss_history)\n",
    "\n",
    "    # Update the model with the changed weights!\n",
    "    if iter % 100 == 0:     \n",
    "        model.save_weights(checkpoint_prefix)\n",
    "    \n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = prepare_train_data(vectorized_string=vec_songs_test, seq_length=seq_length, method='batch', batch_size=batch_size)\n",
    "loss = []\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "for i in tqdm(range(epochs)):\n",
    "    y_hat = model(x_test)\n",
    "    test_loss = compute_loss_wordEmbed(y_test, y_hat)\n",
    "    test_loss = test_loss.numpy().mean()\n",
    "    loss.append(test_loss)\n",
    "test_loss = np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train categorical cross-entropy loss =', train_loss)\n",
    "# print('train perplexity (2**) =', 2**train_loss)\n",
    "# print('train perplexity (exp) =', tf.exp(train_loss).numpy())\n",
    "# print()\n",
    "# print('test categorical cross-entropy loss =', test_loss)\n",
    "# print('test perplexity (2**) =', 2**test_loss)\n",
    "# print('test perplexity (exp) =', tf.exp(test_loss).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating ABC Text (possible songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_wordEmbed(model, start_string, vocab, generation_length):\n",
    "  input_eval = [char2idx(s, vocab) for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  text_generated = []\n",
    "\n",
    "  model.reset_states()\n",
    "  \n",
    "  for i in range(generation_length):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char(predicted_id, vocab))\n",
    "    \n",
    "  text = (start_string + ''.join(text_generated))\n",
    "    \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = build_model_wordEmbed(vocab_size, embedding_dim, lstm_units, batch_size=1)\n",
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "gen_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "n_songs = 10\n",
    "new_songs = []\n",
    "\n",
    "tqdm._instances.clear()\n",
    "with tqdm(total=n_songs) as pbar:\n",
    "    while len(new_songs) < n_songs:\n",
    "        n0 = len(new_songs)\n",
    "        generated_text = generate_text_wordEmbed(gen_model, start_string='X:', vocab=vocab, generation_length=1000)\n",
    "        generated_songs = extract_song_snippet(generated_text)\n",
    "        for song in generated_songs:\n",
    "            new_songs.append(song)\n",
    "        if len(new_songs) > n0:\n",
    "            pbar.update(len(new_songs)-n0)\n",
    "    \n",
    "new_songs = new_songs[0:n_songs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results data in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model.layers[0]\n",
    "layer2 = model.layers[1]\n",
    "layer3 = model.layers[2]\n",
    "\n",
    "model_summary = {\n",
    "    layer1.name: {'type': layer1.__class__.__name__, 'output_shape': layer1.output_shape, 'n_params': layer1.count_params()},\n",
    "    layer2.name: {'type': layer2.__class__.__name__, 'output_shape': layer2.output_shape, 'n_params': layer2.count_params()},\n",
    "    layer3.name: {'type': layer3.__class__.__name__, 'output_shape': layer3.output_shape, 'n_params': layer3.count_params()}\n",
    "}\n",
    "\n",
    "\n",
    "layer1 = gen_model.layers[0]\n",
    "layer2 = gen_model.layers[1]\n",
    "layer3 = gen_model.layers[2]\n",
    "\n",
    "gen_model_summary = {\n",
    "    layer1.name: {'type': layer1.__class__.__name__, 'output_shape': layer1.output_shape, 'n_params': layer1.count_params()},\n",
    "    layer2.name: {'type': layer2.__class__.__name__, 'output_shape': layer2.output_shape, 'n_params': layer2.count_params()},\n",
    "    layer3.name: {'type': layer3.__class__.__name__, 'output_shape': layer3.output_shape, 'n_params': layer3.count_params()}\n",
    "}\n",
    "\n",
    "\n",
    "rslts = {\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'seq_length': seq_length,\n",
    "    'learning_rate': learning_rate,\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'lstm_units': lstm_units,\n",
    "    'original_model (summary)': model_summary,\n",
    "    'train categorical cross-entropy loss': float(train_loss),\n",
    "    'train perplexity (2**)': float(2**train_loss),\n",
    "    'train perplexity (exp)': float(tf.exp(train_loss).numpy()),\n",
    "    'test categorical cross-entropy loss': float(test_loss),\n",
    "    'test perplexity (2**)': float(2**test_loss),\n",
    "    'test perplexity (exp)': float(tf.exp(test_loss).numpy()),\n",
    "    'generative_model (summary)': gen_model_summary,\n",
    "    'generated_songs_abc': new_songs,\n",
    "    'original_model': model.to_json(),\n",
    "    'train loss history (from plot)': [float(loss) for loss in train_loss_history],\n",
    "    'generative_model': gen_model.to_json()\n",
    "}\n",
    "\n",
    "with open(os.path.join(cwd, 'models', 'results.json'), 'w') as f:\n",
    "     f.write(json.dumps(rslts, indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tcc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8423169460a4ab7c0196f3b7f025b4f11bf11307ee8777457d830da4956ded6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
